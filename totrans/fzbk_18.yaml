- en: Probabilistic Grammar Fuzzing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[http://www.fuzzingbook.org/html/ProbabilisticGrammarFuzzer.html](http://www.fuzzingbook.org/html/ProbabilisticGrammarFuzzer.html)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Let us give grammars even more power by assigning *probabilities* to individual
    expansions. This allows us to control how many of each element should be produced,
    and thus allows us to *target* our generated tests towards specific functionality.
    We also show how to learn such probabilities from given sample inputs, and specifically
    direct our tests towards input features that are uncommon in these samples.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Prerequisites**'
  prefs: []
  type: TYPE_NORMAL
- en: You should have read the [chapter on grammars](Grammars.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our implementation hooks into the grammar-based fuzzer introduced in ["Efficient
    Grammar Fuzzing"](GrammarFuzzer.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For learning probabilities from samples, we make use of [parsers](Parser.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synopsis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To [use the code provided in this chapter](Importing.html), write
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: and then make use of the following features.
  prefs: []
  type: TYPE_NORMAL
- en: A *probabilistic* grammar allows attaching individual *probabilities* to production
    rules. To set the probability of an individual expansion `S` to the value `X`
    (between 0 and 1), replace it with a pair
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If we want to ensure that 90% of phone numbers generated have an area code
    starting with `9`, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'A `ProbabilisticGrammarFuzzer` will extract and interpret these options. Here
    is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the large majority of area codes now starts with `9`.
  prefs: []
  type: TYPE_NORMAL
- en: '<svg width="336pt" height="287pt" viewBox="0.00 0.00 336.38 287.00" xmlns:xlink="http://www.w3.org/1999/xlink"><g
    id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 283)"><g
    id="node1" class="node"><title>ProbabilisticGrammarFuzzer</title> <g id="a_node1"><a
    xlink:href="#" xlink:title="class ProbabilisticGrammarFuzzer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A grammar-based fuzzer respecting probabilities in grammars."><text text-anchor="start"
    x="8" y="-56.45" font-family="Patua One, Helvetica, sans-serif" font-weight="bold"
    font-size="14.00" fill="#b03a2e">ProbabilisticGrammarFuzzer</text> <g id="a_node1_0"><a
    xlink:href="#" xlink:title="ProbabilisticGrammarFuzzer"><g id="a_node1_1"><a xlink:href="#"
    xlink:title="check_grammar(self) -> None:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check the grammar passed"><text text-anchor="start" x="26.75" y="-34.25" font-family="''Fira
    Mono'', ''Source Code Pro'', ''Courier'', monospace" font-style="italic" font-size="10.00">check_grammar()</text></a></g>
    <g id="a_node1_2"><a xlink:href="#" xlink:title="choose_node_expansion(self, node:
    DerivationTree, children_alternatives: List[Any]) -> int:'
  prefs: []
  type: TYPE_NORMAL
- en: Return index of expansion in `children_alternatives` to be selected.
  prefs: []
  type: TYPE_NORMAL
- en: '''children_alternatives`: a list of possible children for `node`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Defaults to random. To be overloaded in subclasses."><text text-anchor="start"
    x="26.75" y="-21.5" font-family="''Fira Mono'', ''Source Code Pro'', ''Courier'',
    monospace" font-style="italic" font-size="10.00">choose_node_expansion()</text></a></g>
    <g id="a_node1_3"><a xlink:href="#" xlink:title="supported_opts(self) -> Set[str]:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Set of supported options. To be overloaded in subclasses."><text text-anchor="start"
    x="26.75" y="-8.75" font-family="''Fira Mono'', ''Source Code Pro'', ''Courier'',
    monospace" font-style="italic" font-size="10.00">supported_opts()</text></a></g></a></g></a></g></g>
    <g id="node2" class="node"><title>GrammarFuzzer</title> <g id="a_node2"><a xlink:href="GrammarFuzzer.html"
    xlink:title="class GrammarFuzzer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Produce strings from grammars efficiently, using derivation trees."><text text-anchor="start"
    x="45.88" y="-165.7" font-family="Patua One, Helvetica, sans-serif" font-weight="bold"
    font-size="14.00" fill="#b03a2e">GrammarFuzzer</text> <g id="a_node2_4"><a xlink:href="#"
    xlink:title="GrammarFuzzer"><g id="a_node2_5"><a xlink:href="GrammarFuzzer.html"
    xlink:title="__init__(self, grammar: Dict[str, List[Expansion]], start_symbol:
    str = ''<start>'', min_nonterminals: int = 0, max_nonterminals: int = 10, disp:
    bool = False, log: Union[bool, int] = False) -> None:'
  prefs: []
  type: TYPE_NORMAL
- en: Produce strings from `grammar`, starting with `start_symbol`.
  prefs: []
  type: TYPE_NORMAL
- en: If `min_nonterminals` or `max_nonterminals` is given, use them as limits
  prefs: []
  type: TYPE_NORMAL
- en: for the number of nonterminals produced.
  prefs: []
  type: TYPE_NORMAL
- en: If `disp` is set, display the intermediate derivation trees.
  prefs: []
  type: TYPE_NORMAL
- en: 'If `log` is set, show intermediate steps as text on standard output."><text
    text-anchor="start" x="62.75" y="-143.5" font-family="''Fira Mono'', ''Source
    Code Pro'', ''Courier'', monospace" font-weight="bold" font-style="italic" font-size="10.00">__init__()</text></a></g>
    <g id="a_node2_6"><a xlink:href="GrammarFuzzer.html" xlink:title="fuzz(self) ->
    str:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Produce a string from the grammar."><text text-anchor="start" x="62.75" y="-130.75"
    font-family="''Fira Mono'', ''Source Code Pro'', ''Courier'', monospace" font-weight="bold"
    font-style="italic" font-size="10.00">fuzz()</text></a></g> <g id="a_node2_7"><a
    xlink:href="GrammarFuzzer.html" xlink:title="fuzz_tree(self) -> DerivationTree:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Produce a derivation tree from the grammar."><text text-anchor="start" x="62.75"
    y="-118" font-family="''Fira Mono'', ''Source Code Pro'', ''Courier'', monospace"
    font-weight="bold" font-size="10.00">fuzz_tree()</text></a></g></a></g></a></g></g>
    <g id="edge1" class="edge"><title>ProbabilisticGrammarFuzzer->GrammarFuzzer</title></g>
    <g id="node3" class="node"><title>Fuzzer</title> <g id="a_node3"><a xlink:href="Fuzzer.html"
    xlink:title="class Fuzzer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Base class for fuzzers."><text text-anchor="start" x="75.12" y="-262.2" font-family="Patua
    One, Helvetica, sans-serif" font-weight="bold" font-size="14.00" fill="#b03a2e">Fuzzer</text>
    <g id="a_node3_8"><a xlink:href="#" xlink:title="Fuzzer"><g id="a_node3_9"><a
    xlink:href="Fuzzer.html" xlink:title="run(self, runner: Fuzzer.Runner = <Fuzzer.Runner
    object>) -> Tuple[subprocess.CompletedProcess, str]:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run `runner` with fuzz input"><text text-anchor="start" x="77.75" y="-240"
    font-family="''Fira Mono'', ''Source Code Pro'', ''Courier'', monospace" font-weight="bold"
    font-size="10.00">run()</text></a></g> <g id="a_node3_10"><a xlink:href="Fuzzer.html"
    xlink:title="runs(self, runner: Fuzzer.Runner = <Fuzzer.PrintRunner object>, trials:
    int = 10) -> List[Tuple[subprocess.CompletedProcess, str]]:'
  prefs: []
  type: TYPE_NORMAL
- en: Run `runner` with fuzz input, `trials` times"><text text-anchor="start" x="77.75"
    y="-227.25" font-family="'Fira Mono', 'Source Code Pro', 'Courier', monospace"
    font-weight="bold" font-size="10.00">runs()</text></a></g></a></g></a></g></g>
    <g id="edge2" class="edge"><title>GrammarFuzzer->Fuzzer</title></g> <g id="node4"
    class="node"><title>Legend</title> <text text-anchor="start" x="209.12" y="-52.62"
    font-family="Patua One, Helvetica, sans-serif" font-weight="bold" font-size="10.00"
    fill="#b03a2e">Legend</text> <text text-anchor="start" x="209.12" y="-42.62" font-family="Patua
    One, Helvetica, sans-serif" font-size="10.00">• </text> <text text-anchor="start"
    x="215.12" y="-42.62" font-family="'Fira Mono', 'Source Code Pro', 'Courier',
    monospace" font-weight="bold" font-size="8.00">public_method()</text> <text text-anchor="start"
    x="209.12" y="-32.62" font-family="Patua One, Helvetica, sans-serif" font-size="10.00">• </text>
    <text text-anchor="start" x="215.12" y="-32.62" font-family="'Fira Mono', 'Source
    Code Pro', 'Courier', monospace" font-size="8.00">private_method()</text> <text
    text-anchor="start" x="209.12" y="-22.62" font-family="Patua One, Helvetica, sans-serif"
    font-size="10.00">• </text> <text text-anchor="start" x="215.12" y="-22.62" font-family="'Fira
    Mono', 'Source Code Pro', 'Courier', monospace" font-style="italic" font-size="8.00">overloaded_method()</text>
    <text text-anchor="start" x="209.12" y="-13.57" font-family="Helvetica,sans-Serif"
    font-size="9.00">Hover over names to see doc</text></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: The Law of Leading Digits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In all our examples so far, you may have noted that inputs generated by a program
    differ quite a bit from "natural" inputs as they occur in real life. This is true
    even for innocuous elements such as numbers – yes, the numbers we have generated
    so far actually *differ* from numbers in the real world. This is because in real-life
    sets of numerical data, the *leading significant digit* is likely to be small:
    Actually, on average, the leading digit `1` occurs more than *six times* as often
    as the leading digit `8` or `9`. It has been shown that this result applies to
    a wide variety of data sets, including electricity bills, street addresses, stock
    prices, house prices, population numbers, death rates, lengths of rivers, physical
    and mathematical constants (Wikipedia).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This law of leading digits was first observed by Newcomb [[Simon Newcomb, 1881](http://www.jstor.org/stable/2369148)]
    and later formalized by Benford in [[Frank Benford, 1938](http://links.jstor.org/sici?sici=0003-049X%2819380331%2978%3A4%3C551%3ATLOAN%3E2.0.CO%3B2-G)].
    Let us take a look at the conditions that determine the first digit of a number.
    We can easily compute the first digit by converting the number into a string and
    take the first character:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: To do this mathematically, though, we have to take the fractional part of their
    logarithm, or formally
  prefs: []
  type: TYPE_NORMAL
- en: $$ d = 10^{\{\log_{10}(x)\}} $$
  prefs: []
  type: TYPE_NORMAL
- en: where $\{x\}$ is the fractional part of $x$ (i.e. $\{1.234\} = 0.234$).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Most sets of "naturally" occurring numbers should not have any bias in the fractional
    parts of their logarithms, and hence, the fractional part $\{\log_{10}(x)\}$ is
    typically uniformly distributed. However, the fractional parts for the individual
    digits are *not* evenly distributed.
  prefs: []
  type: TYPE_NORMAL
- en: For a number to start with a digit $d$, the condition $d < 10^{\{\log_{10}(x)\}}
    < d + 1$ must hold. To start with the digit 1, the fractional part $\{\log_{10}(x)\}$
    must thus be in the range
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: To start with the digit 2, though, it must be in the range
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'which is much smaller. Formally, the probability $P(d)$ for a leading digit
    $d$ (again, assuming uniformly distributed fractional parts) is known as Benford''s
    law: $$ P(d) = \log_{10}(d + 1) - \log_{10}(d) $$ which gives us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us compute these probabilities for all digits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/209633c7fc0e1c06b36760d52f9e7433.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that a leading 1 is indeed six times as probable as a leading 9.
  prefs: []
  type: TYPE_NORMAL
- en: Benford's law has a number of applications. Most notably, it can be used to
    detect "non-natural" numbers, i.e. numbers that apparently were created randomly
    rather than coming from a "natural" source. if you write a scientific paper and
    fake data by putting in random numbers (for instance, [using our grammar fuzzer](GrammarFuzzer.html)
    on integers), you will likely violate Benford's law, and this can indeed be spotted.
    On the other hand, how would we proceed if we *wanted* to create numbers that
    adhere to Benford's law? To this end, we need to be able to *encode* probabilities
    such as the above in our grammar, such that we can ensure that a leading digit
    is indeed a `1` in 30% of all cases.
  prefs: []
  type: TYPE_NORMAL
- en: Specifying Probabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of this chapter is to assign *probabilities* to individual expansions
    in the grammar, such that we can express that some expansion alternatives should
    be favored over others. This is not only useful to generate "natural"-looking
    numbers, but even more so to *direct* test generation towards a specific goal.
    If you recently have changed some code in your program, you would probably like
    to generate inputs that exercise precisely this code. By raising the probabilities
    on the input elements associated with the changed code, you will get more tests
    that exercise the changed code.
  prefs: []
  type: TYPE_NORMAL
- en: Our concept for expressing probabilities is to *annotate* individual expansions
    with attributes such as probabilities, using the annotation mechanism introduced
    in [the chapter on grammars](Grammars.html). To this end, we allow that an expansion
    cannot only be a string, but also a *pair* of a string and a set of attributes,
    as in
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `opts()` function would allow us to express probabilities for choosing
    the individual expansions. The addition would have a probability of 10%, the subtraction
    of 20%. The remaining probability (in this case 70%) is equally distributed over
    the non-attributed expansions (in this case the single last one).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now use pairs with `opts()` to assign probabilities to our expression
    grammar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This is how the grammar expansions are represented internally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'However, we typically access the expansion string and the associated probability
    via designated helper functions, `exp_string()` (from the [chapter on Grammars](Grammars.html))
    and `exp_prob()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Our existing fuzzers are all set up to work with grammars annotated this way.
    They simply ignore all annotations.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Computing Probabilities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us define functions that access probabilities for given expansions. While
    doing so, they also check for inconsistencies.
  prefs: []
  type: TYPE_NORMAL
- en: Distributing Probabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here is how we distribute probabilities for expansions without specified probabilities.
    Given an expansion rule
  prefs: []
  type: TYPE_NORMAL
- en: '$$S ::= a_1\:|\: a_2 \:|\: \dots \:|\: a_n \:|\: u_1 \:|\: u_2 \:|\: \dots
    u_m$$'
  prefs: []
  type: TYPE_NORMAL
- en: with $n \ge 0$ alternatives $a_i$ for which the probability $p(a_i)$ is *specified*
    and $m \ge 0$ alternatives $u_j$ for which the probability $p(u_j)$ is *unspecified*,
    the "remaining" probability is distributed equally over all $u_j$; in other words,
  prefs: []
  type: TYPE_NORMAL
- en: $$p(u_j) = \frac{1 - \sum_{i = 1}^{n}p(a_i)}{m}$$
  prefs: []
  type: TYPE_NORMAL
- en: If no probabilities are specified ($n = 0$), then all expansions have the same
    probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overall sum of probabilities must be 1:'
  prefs: []
  type: TYPE_NORMAL
- en: $$\sum_{i = 1}^{n} p(a_i) + \sum_{j = 1}^{m} p(u_i) = 1$$
  prefs: []
  type: TYPE_NORMAL
- en: We check these properties while distributing probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The function `exp_probabilities()` returns a mapping of all expansions in a
    rule to their respective probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: The gist of `exp_probabilities()` is handled in `prob_distribution()`, which
    does the actual checking and computation.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the mapping `exp_probabilities()` returns for the annotated `<leaddigit>`
    element:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: If no expansion is annotated, all expansions have the same likelihood of being
    selected, as in our previous grammar fuzzers.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s how `exp_probabilities()` distributes any remaining probability across
    non-annotated expansions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Checking Probabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can use the checking capabilities of `exp_probabilities()` to check a probabilistic
    grammar for consistency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Expanding by Probability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have seen how to specify probabilities for a grammar, we can actually
    implement probabilistic expansion. In our `ProbabilisticGrammarFuzzer`, it suffices
    to overload one method, namely `choose_node_expansion()`. For each of the children
    we can choose from (typically all expansions of a symbol), we determine their
    probability (using `exp_probabilities()` defined above), and make a weighted choice
    using `random.choices()` with a `weight` argument.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Our probabilistic grammar fuzzer works just like the non-probabilistic grammar
    fuzzer, except that it actually respects probability annotations. Let us generate
    a couple of "natural" numbers that respect Benford''s law:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'In contrast, these numbers are pure random:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Are the "natural" numbers really more "natural" than the random ones? To show
    that `ProbabilisticGrammarFuzzer` indeed respects the probabilistic annotations,
    let us create a specific fuzzer for the lead digit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'If we generate thousands of lead digits, their distribution should again follow
    Benford''s law:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Quod erat demonstrandum! The distribution is pretty much exactly as originally
    specified. We now have a fuzzer where we can exercise control by specifying probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Directed Fuzzing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Assigning probabilities to individual expansions gives us great control over
    which inputs should be generated. By choosing probabilities wisely, we can *direct*
    fuzzing towards specific functions and features – for instance, towards functions
    that are particularly critical, prone to failures, or that have been recently
    changed.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, consider the URL grammar from the [chapter on grammars](Grammars.html).
    Let us assume we have just made a change to our implementation of the secure FTP
    protocol. By assigning a higher probability to the `ftps` scheme, we can generate
    more URLs that will specifically test this functionality.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let us define a helper function that sets a particular option:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a specialization just for probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us use `set_prob()` to give the `ftps` expansion a probability of 80%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'If we use this grammar for fuzzing, we will get plenty of `ftps:` prefixes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: In a similar vein, we can direct URL generation towards specific hosts or ports;
    we can favor URLs with queries, fragments, or logins – or URLs without these.
    All it takes is to set appropriate probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'By setting the probability of an expansion to zero, we can effectively disable
    specific expansions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Note that even if we set the probability of an expansion to zero, we may still
    see the expansion taken. This can happen during the "closing" phase of [our grammar
    fuzzer](GrammarFuzzer.html), when the expansion is closed at minimum cost. At
    this stage, even expansions with "zero" probability will be taken if this is necessary
    for closing the expansion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us illustrate this feature using the `<expr>` rule from our expression
    grammar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: If we set the probability of the `<term>` expansion to zero, the string should
    expand again and again.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: Still, in the "closing" phase, subexpressions will eventually expand into `<term>`,
    as it is the only way to close the expansion. Tracking `choose_node_expansion()`
    shows that it is invoked with only one possible expansion `<term>`, which has
    to be taken even though its specified probability is zero.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Probabilities in Context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While specified probabilities give us a means to control which expansions are
    taken how often, this control by itself may not be enough. As an example, consider
    the following grammar for IPv4 addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'We can easily use this grammar to create IP addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if we want to assign a specific probability to one of the four octets,
    we are out of luck. All we can do is to assign the same probability distribution
    for all four octets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: If we want to assign *different* probabilities to each of the four octets, what
    do we do?
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer lies in the concept of *context*, which we already have seen [while
    discussing coverage-driven fuzzers](GrammarCoverageFuzzer.html). As with coverage-driven
    fuzzing, the idea is to *duplicate* the element whose probability we want to set
    dependent on its context. In our case, this means to duplicate the `<octet>` element
    to four individual ones, each of which can then get an individual probability
    distribution. We can do this programmatically, using the `duplicate_context()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now assign different probabilities to each of the `<octet>` symbols.
    For instance, we can force specific expansions by setting their probability to
    100%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'The remaining two octets `<octet-3>` and `<octet-4>` have no specific probabilities
    set. During fuzzing, all their expansions (all octets) are thus still available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: Just as with coverage, we can duplicate grammar rules arbitrarily often to get
    more and more finer-grained control over probabilities. However, this finer-grained
    control also comes at the cost of having to maintain these probabilities. In the
    next section, we will therefore discuss means to assign and tune such probabilities
    automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Learning Probabilities from Samples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Probabilities need not be set manually all the time. They can also be *learned*
    from other sources, notably by counting *how frequently individual expansions
    occur in a given set of inputs*. This is useful in a number of situations, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Test *common* features. The idea is that during testing, one may want to focus
    on frequently occurring (or frequently used) features first, to ensure correct
    functionality for the most common usages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Test *uncommon* features. Here, the idea is to have test generation focus on
    features that are rarely seen (or not seen at all) in inputs. This is the same
    motivation as with [grammar coverage](GrammarCoverageFuzzer.html), but from a
    probabilistic standpoint.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Focus on specific *slices*. One may have a set of inputs that is of particular
    interest (for instance, because they exercise a critical functionality, or recently
    have discovered bugs). Using this learned distribution for fuzzing allows us to
    *focus* on precisely these functionalities of interest.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let us first introduce counting expansions and learning probabilities, and then
    detail these scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Counting Expansions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We start with implementing a means to take a set of inputs and determine the
    number of expansions in that set. To this end, we need the *parsers* introduced
    [in the previous chapter](Parser.html) to transform a string input into a derivation
    tree. For our IP address grammar, this is how this works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: <svg width="375pt" height="173pt" viewBox="0.00 0.00 374.75 173.00" xmlns:xlink="http://www.w3.org/1999/xlink"><g
    id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 169)"><g
    id="node1" class="node"><title>0</title> <text text-anchor="middle" x="183.38"
    y="-151.7" font-family="Times,serif" font-size="14.00"><start></text></g> <g id="node2"
    class="node"><title>1</title> <text text-anchor="middle" x="183.38" y="-101.45"
    font-family="Times,serif" font-size="14.00"><address></text></g> <g id="edge1"
    class="edge"><title>0->1</title></g> <g id="node3" class="node"><title>2</title>
    <text text-anchor="middle" x="21.38" y="-51.2" font-family="Times,serif" font-size="14.00"><octet></text></g>
    <g id="edge2" class="edge"><title>1->2</title></g> <g id="node5" class="node"><title>4</title>
    <text text-anchor="middle" x="75.38" y="-51.2" font-family="Times,serif" font-size="14.00">.
    (46)</text></g> <g id="edge4" class="edge"><title>1->4</title></g> <g id="node6"
    class="node"><title>5</title> <text text-anchor="middle" x="129.38" y="-51.2"
    font-family="Times,serif" font-size="14.00"><octet></text></g> <g id="edge5" class="edge"><title>1->5</title></g>
    <g id="node8" class="node"><title>7</title> <text text-anchor="middle" x="183.38"
    y="-51.2" font-family="Times,serif" font-size="14.00">. (46)</text></g> <g id="edge7"
    class="edge"><title>1->7</title></g> <g id="node9" class="node"><title>8</title>
    <text text-anchor="middle" x="237.38" y="-51.2" font-family="Times,serif" font-size="14.00"><octet></text></g>
    <g id="edge8" class="edge"><title>1->8</title></g> <g id="node11" class="node"><title>10</title>
    <text text-anchor="middle" x="291.38" y="-51.2" font-family="Times,serif" font-size="14.00">.
    (46)</text></g> <g id="edge10" class="edge"><title>1->10</title></g> <g id="node12"
    class="node"><title>11</title> <text text-anchor="middle" x="345.38" y="-51.2"
    font-family="Times,serif" font-size="14.00"><octet></text></g> <g id="edge11"
    class="edge"><title>1->11</title></g> <g id="node4" class="node"><title>3</title>
    <text text-anchor="middle" x="21.38" y="-0.95" font-family="Times,serif" font-size="14.00">127</text></g>
    <g id="edge3" class="edge"><title>2->3</title></g> <g id="node7" class="node"><title>6</title>
    <text text-anchor="middle" x="129.38" y="-0.95" font-family="Times,serif" font-size="14.00">0
    (48)</text></g> <g id="edge6" class="edge"><title>5->6</title></g> <g id="node10"
    class="node"><title>9</title> <text text-anchor="middle" x="237.38" y="-0.95"
    font-family="Times,serif" font-size="14.00">0 (48)</text></g> <g id="edge9" class="edge"><title>8->9</title></g>
    <g id="node13" class="node"><title>12</title> <text text-anchor="middle" x="345.38"
    y="-0.95" font-family="Times,serif" font-size="14.00">1 (49)</text></g> <g id="edge12"
    class="edge"><title>11->12</title></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: In a tree such as this one, we can now *count* individual expansions. In the
    above tree, for instance, we have two expansions of `<octet>` into `0`, one into
    `1`, and one into `127`. The expansion `<octet>` into `0` makes up 50% of all
    expansions seen; the expansions into `127` and `1` make up 25% each, and the other
    ones 0%. These are the probabilities we'd like to assign to our "learned" grammar.
  prefs: []
  type: TYPE_NORMAL
- en: We introduce a class `ExpansionCountMiner` which allows us to count how frequently
    individual expansions take place. Its initialization method takes a parser (say,
    an `EarleyParser`) that would be initialized with the appropriate grammar.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: The attribute `expansion_counts` holds the expansions seen; adding a tree with
    `add_tree()` traverses the given tree and adds all expansions seen.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: The method `count_expansions()` is the one facing the public; it takes a list
    of inputs, parses them, and processes the resulting trees. The method `counts()`
    returns the counts found.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us try this out on our IP address grammar. We create an `ExpansionCountMiner`
    for our IP address grammar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'We parse a (small) set of IP addresses and count the expansions occurring:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: You see that we have one expansion into `127`, and two into `0`. These are the
    counts we can use to assign probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Assigning Probabilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The distribution of counts, as determined by `ExpansionCountMiner` is what we
    can use to assign probabilities to our grammar. To this end, we introduce a subclass
    `ProbabilisticGrammarMiner` whose method `set_expansion_probabilities()` processes
    all expansions of a given symbol, checks whether it occurs in a given count distribution,
    and assigns probabilities using the following formula.
  prefs: []
  type: TYPE_NORMAL
- en: Given a set $T$ of derivation trees (as mined from samples), we determine the
    probabilities $p_i$ for each alternative $a_i$ of a symbol $S \rightarrow a_1
    | \dots | a_n$ as
  prefs: []
  type: TYPE_NORMAL
- en: $$p_i = \frac{\text{Expansions of $S \rightarrow a_i$ in $T$}}{\text{Expansions
    of $S$ in $T$}}$$
  prefs: []
  type: TYPE_NORMAL
- en: Should $S$ not occur at all in $T$, then $p_i$ is *unspecified*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the implementation of `set_expansion_probabilities()`, implementing
    the above formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: The typical use of `ProbabilisticGrammarMiner` is through `mine_probabilistic_grammar()`,
    which first determines a distribution from a set of inputs, and then sets the
    probabilities accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us put this to use. We create a grammar miner for IP addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'We now use `mine_probabilistic_grammar()` to mine the grammar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s the resulting distribution of octets in our grammar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: 'If we use these probabilities for fuzzing, we will get the same distribution
    of octets as in our sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: By learning from a sample, we can thus adjust our fuzzing towards the (syntactic)
    properties of this very sample.
  prefs: []
  type: TYPE_NORMAL
- en: Testing Common Features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let us now get to our three usage scenarios. The first scenario is to create
    probability distributions right out of a sample, and to use these very distributions
    during test generation. This helps to focus test generation on those features
    that are *most commonly used*, which thus minimizes the risk of customers encountering
    failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate testing of common features, we choose the URL domain. Let us
    assume that we are running some Web-related service, and this is a sample of the
    URLs our customers access most:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: Using the Earley parser from the [chapter on parsers](Parser.html), we can parse
    any of these inputs into a parse tree; we have to specify a token set, though.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: '<svg width="536pt" height="374pt" viewBox="0.00 0.00 536.38 374.00" xmlns:xlink="http://www.w3.org/1999/xlink"><g
    id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 370)"><g
    id="node1" class="node"><title>0</title> <text text-anchor="middle" x="151.88"
    y="-352.7" font-family="Times,serif" font-size="14.00"><start></text></g> <g id="node2"
    class="node"><title>1</title> <text text-anchor="middle" x="151.88" y="-302.45"
    font-family="Times,serif" font-size="14.00"><url></text></g> <g id="edge1" class="edge"><title>0->1</title></g>
    <g id="node3" class="node"><title>2</title> <text text-anchor="middle" x="28.88"
    y="-252.2" font-family="Times,serif" font-size="14.00"><scheme></text></g> <g
    id="edge2" class="edge"><title>1->2</title></g> <g id="node5" class="node"><title>4</title>
    <text text-anchor="middle" x="95.88" y="-252.2" font-family="Times,serif" font-size="14.00">://</text></g>
    <g id="edge4" class="edge"><title>1->4</title></g> <g id="node6" class="node"><title>5</title>
    <text text-anchor="middle" x="151.88" y="-252.2" font-family="Times,serif" font-size="14.00"><authority></text></g>
    <g id="edge5" class="edge"><title>1->5</title></g> <g id="node12" class="node"><title>11</title>
    <text text-anchor="middle" x="229.88" y="-252.2" font-family="Times,serif" font-size="14.00"><path></text></g>
    <g id="edge11" class="edge"><title>1->11</title></g> <g id="node16" class="node"><title>15</title>
    <text text-anchor="middle" x="326.88" y="-252.2" font-family="Times,serif" font-size="14.00"><query></text></g>
    <g id="edge15" class="edge"><title>1->15</title></g> <g id="node4" class="node"><title>3</title>
    <text text-anchor="middle" x="21.88" y="-201.95" font-family="Times,serif" font-size="14.00">https</text></g>
    <g id="edge3" class="edge"><title>2->3</title></g> <g id="node7" class="node"><title>6</title>
    <text text-anchor="middle" x="72.88" y="-201.95" font-family="Times,serif" font-size="14.00"><host></text></g>
    <g id="edge6" class="edge"><title>5->6</title></g> <g id="node9" class="node"><title>8</title>
    <text text-anchor="middle" x="125.88" y="-201.95" font-family="Times,serif" font-size="14.00">:
    (58)</text></g> <g id="edge8" class="edge"><title>5->8</title></g> <g id="node10"
    class="node"><title>9</title> <text text-anchor="middle" x="177.88" y="-201.95"
    font-family="Times,serif" font-size="14.00"><port></text></g> <g id="edge9" class="edge"><title>5->9</title></g>
    <g id="node8" class="node"><title>7</title> <text text-anchor="middle" x="72.88"
    y="-151.7" font-family="Times,serif" font-size="14.00">cispa.saarland</text></g>
    <g id="edge7" class="edge"><title>6->7</title></g> <g id="node11" class="node"><title>10</title>
    <text text-anchor="middle" x="177.88" y="-151.7" font-family="Times,serif" font-size="14.00">80</text></g>
    <g id="edge10" class="edge"><title>9->10</title></g> <g id="node13" class="node"><title>12</title>
    <text text-anchor="middle" x="229.88" y="-201.95" font-family="Times,serif" font-size="14.00">/
    (47)</text></g> <g id="edge12" class="edge"><title>11->12</title></g> <g id="node14"
    class="node"><title>13</title> <text text-anchor="middle" x="276.88" y="-201.95"
    font-family="Times,serif" font-size="14.00"><id></text></g> <g id="edge13" class="edge"><title>11->13</title></g>
    <g id="node15" class="node"><title>14</title> <text text-anchor="middle" x="275.88"
    y="-151.7" font-family="Times,serif" font-size="14.00">def</text></g> <g id="edge14"
    class="edge"><title>13->14</title></g> <g id="node17" class="node"><title>16</title>
    <text text-anchor="middle" x="326.88" y="-201.95" font-family="Times,serif" font-size="14.00">?
    (63)</text></g> <g id="edge16" class="edge"><title>15->16</title></g> <g id="node18"
    class="node"><title>17</title> <text text-anchor="middle" x="389.88" y="-201.95"
    font-family="Times,serif" font-size="14.00"><params></text></g> <g id="edge17"
    class="edge"><title>15->17</title></g> <g id="node19" class="node"><title>18</title>
    <text text-anchor="middle" x="327.88" y="-151.7" font-family="Times,serif" font-size="14.00"><param></text></g>
    <g id="edge18" class="edge"><title>17->18</title></g> <g id="node26" class="node"><title>25</title>
    <text text-anchor="middle" x="389.88" y="-151.7" font-family="Times,serif" font-size="14.00">&
    (38)</text></g> <g id="edge25" class="edge"><title>17->25</title></g> <g id="node27"
    class="node"><title>26</title> <text text-anchor="middle" x="460.88" y="-151.7"
    font-family="Times,serif" font-size="14.00"><params></text></g> <g id="edge26"
    class="edge"><title>17->26</title></g> <g id="node20" class="node"><title>19</title>
    <text text-anchor="middle" x="276.88" y="-101.45" font-family="Times,serif" font-size="14.00"><id></text></g>
    <g id="edge19" class="edge"><title>18->19</title></g> <g id="node22" class="node"><title>21</title>
    <text text-anchor="middle" x="325.88" y="-101.45" font-family="Times,serif" font-size="14.00">=
    (61)</text></g> <g id="edge21" class="edge"><title>18->21</title></g> <g id="node23"
    class="node"><title>22</title> <text text-anchor="middle" x="377.88" y="-101.45"
    font-family="Times,serif" font-size="14.00"><nat></text></g> <g id="edge22" class="edge"><title>18->22</title></g>
    <g id="node21" class="node"><title>20</title> <text text-anchor="middle" x="276.88"
    y="-51.2" font-family="Times,serif" font-size="14.00">def</text></g> <g id="edge20"
    class="edge"><title>19->20</title></g> <g id="node24" class="node"><title>23</title>
    <text text-anchor="middle" x="364.88" y="-51.2" font-family="Times,serif" font-size="14.00"><digit></text></g>
    <g id="edge23" class="edge"><title>22->23</title></g> <g id="node25" class="node"><title>24</title>
    <text text-anchor="middle" x="364.88" y="-0.95" font-family="Times,serif" font-size="14.00">7
    (55)</text></g> <g id="edge24" class="edge"><title>23->24</title></g> <g id="node28"
    class="node"><title>27</title> <text text-anchor="middle" x="462.88" y="-101.45"
    font-family="Times,serif" font-size="14.00"><param></text></g> <g id="edge27"
    class="edge"><title>26->27</title></g> <g id="node29" class="node"><title>28</title>
    <text text-anchor="middle" x="416.88" y="-51.2" font-family="Times,serif" font-size="14.00"><id></text></g>
    <g id="edge28" class="edge"><title>27->28</title></g> <g id="node31" class="node"><title>30</title>
    <text text-anchor="middle" x="465.88" y="-51.2" font-family="Times,serif" font-size="14.00">=
    (61)</text></g> <g id="edge30" class="edge"><title>27->30</title></g> <g id="node32"
    class="node"><title>31</title> <text text-anchor="middle" x="514.88" y="-51.2"
    font-family="Times,serif" font-size="14.00"><id></text></g> <g id="edge31" class="edge"><title>27->31</title></g>
    <g id="node30" class="node"><title>29</title> <text text-anchor="middle" x="416.88"
    y="-0.95" font-family="Times,serif" font-size="14.00">x23</text></g> <g id="edge29"
    class="edge"><title>28->29</title></g> <g id="node33" class="node"><title>32</title>
    <text text-anchor="middle" x="514.88" y="-0.95" font-family="Times,serif" font-size="14.00">abc</text></g>
    <g id="edge32" class="edge"><title>31->32</title></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us apply our `ProbabilisticGrammarMiner` class on these inputs, using the
    above `url_parser` parser, and obtain a probabilistic URL grammar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: 'These are the counts we obtained during parsing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: These counts translate into individual probabilities. We see that in our sample,
    most URLs use the `https:` scheme, whereas there is no input using the `ftp:`
    scheme.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: 'Likewise, we see that most given URLs have multiple parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: When we use this probabilistic grammar for fuzzing, these distributions are
    reflected in our generated inputs – no `ftp:` schemes either, and most inputs
    have multiple parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: 'Being able to replicate a probability distribution learned from a sample is
    not only important for focusing on commonly used features. It can also help in
    achieving *valid inputs*, in particular if one learns probabilities *in context*,
    as discussed above: If within a given context, some elements are more likely than
    others (because they depend on each other), a learned probability distribution
    will reflect this; and hence, inputs generated from this learned probability distribution
    will have a higher chance to be valid, too. We will explore this further in the
    [exercises](#Exercises), below.'
  prefs: []
  type: TYPE_NORMAL
- en: Testing Uncommon Features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have focused on *common* features; but from a testing perspective,
    one may just as well test *uncommon* features – that is, features that rarely
    occur in our usage samples and therefore would be less exercised in practice.
    This is a common scenario in security testing, where one focuses on uncommon (and
    possibly lesser-known) features, as fewer users means fewer bugs reported, and
    thus more bugs left to be found and exploited.
  prefs: []
  type: TYPE_NORMAL
- en: 'To have our probabilistic grammar fuzzer focus on *uncommon* features, we *change
    the learned probabilities* such that commonly occurring features (i.e., those
    with a high learned probability) get a low probability, and vice versa: The last
    shall be first, and the first last. A particularly simple way to achieve such
    an *inversion* of probabilities is to *swap* them: The alternatives with the highest
    and lowest probability swaps their probabilities, as so the alternatives with
    the second-highest and second-lowest probability, the alternatives with the third
    highest and lowest, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: The function `invert_expansion()` takes an expansion (a list of alternatives)
    from a grammar and returns a new inverted expansion in which the probabilities
    have been swapped according to the rule above. It creates a list of indexes, sorts
    it by increasing probability, and then for each $n$-th element, assigns it the
    probability of the $n$-th last element in the indexes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: 'Here''s `invert_expansion()` in action. This is our original probability distribution
    for URL schemes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: And this is the "inverted" distribution. We see that the `ftp:` scheme, which
    previously had a probability of zero, now has the highest probability, whereas
    the most common scheme, `https:`, now has the previous zero probability of the
    `ftp:` scheme.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: 'One nice feature of this swapping of probabilities is that the sum of probabilities
    stays unchanged; no normalization is needed. Another nice feature is that the
    inversion of the inversion returns the original distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that our implementation does not universally satisfy this property: If
    two alternatives $a_1$ and $a_2$ in the expansion share the same probability,
    then the second inversion may assign different probabilities to $a_1$ and $a_2$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can apply this inversion of expansions across the entire grammar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that probabilities would be swapped for each and every expansion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: If we now use this "inverted" grammar for fuzzing, the generated inputs will
    focus on the *complement of the input samples*. We will get plenty of tests of
    user/password features, as well as `ftp:` schemes – in essence, all the features
    present in our language, but rarely used (if at all) in our input samples.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: Besides having *only* common or *only* uncommon features, one can also create
    mixed forms – for instance, testing uncommon features in a common context. This
    can be helpful for security testing, where one may want an innocuous (common)
    "envelope" combined with an (uncommon) "payload". It all depends on where and
    how we tune the probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Learning Probabilities from Input Slices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our previous examples, we have learned from *all* inputs to generate common
    or uncommon inputs. However, we can also learn from a *subset* of inputs to focus
    on the features present in that subset (or, conversely, to *avoid* its features).
    If we know, for instance, that there is some subset of inputs that covers a functionality
    of interest (say, because it is particularly critical or because it has been recently
    changed), we can learn from this very subset and focus our test generation on
    its features.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate this approach, let us use the CGI grammar introduced in the [chapter
    on coverage](Coverage.html). We have a special interest in Line 25 in our CGI
    decoder – that is, the line that processes a `%` character followed by two valid
    hexadecimal digits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: Let us assume that we do not know precisely under which conditions Line 25 is
    executed – but still, we'd like to test it thoroughly. With our probability learning
    tools, we can learn these conditions, though. We start with a set of random inputs
    and consider the subset that covers Line 25.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: 'These are all the random inputs that cover Line 25:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: 'Actually, about half of the inputs cover Line 25:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE159]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us now learn a probabilistic grammar from this slice of inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE161]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that percentage signs are very likely to occur:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE162]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE163]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this grammar, we can now generate tests that specifically target Line
    25:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE164]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE165]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE166]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that the fraction of inputs that cover Line 25 is much higher already,
    showing that our focusing works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE167]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE168]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE169]'
  prefs: []
  type: TYPE_PRE
- en: 'Repeating this one more time yields an even higher focusing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE170]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE171]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE172]'
  prefs: []
  type: TYPE_PRE
- en: By learning (and re-learning) probabilities from a subset of sample inputs,
    we can *specialize* fuzzers towards the properties of that subset – in our case,
    inputs that contain percentage signs and valid hexadecimal letters. The degree
    to which we can specialize things is induced by the number of variables we can
    control – in our case, the probabilities for the individual rules. Adding more
    context to the grammar, as discussed above, will increase the number of variables,
    and thus the amount of specialization.
  prefs: []
  type: TYPE_NORMAL
- en: A high degree of specialization, however, limits our possibilities to explore
    combinations that fall *outside* the selected scope, and limit our possibilities
    to find bugs induced by these combinations. This trade-off is known as *exploration
    vs. exploitation* in machine learning – shall one try to explore as many (possibly
    shallow) combinations as possible, or focus (exploit) specific areas? In the end,
    it all depends on where the bugs are, and where we are most likely to find them.
    Assigning and learning probabilities allows us to control the search strategies
    – from the common to the uncommon to specific subsets.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting Unnatural Numbers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let us close this chapter by getting back to our introductory example. We said
    that Benford's law allows us not only to produce, but also to detect "unnatural"
    lead digit distributions such as the ones produced by simple random choices.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we use the regular `GrammarFuzzer` class (which ignores probabilities) to
    generate (random) lead digits, this is the distribution we get for each leading
    digit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE173]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE174]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE175]'
  prefs: []
  type: TYPE_PRE
- en: (For simplicity, we use the simple list `count()` method here rather than deploying
    the full-fledged `ProbabilisticGrammarMiner`.)
  prefs: []
  type: TYPE_NORMAL
- en: 'If we had a natural distribution of lead digits, this is what we would expect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE176]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE177]'
  prefs: []
  type: TYPE_PRE
- en: 'And if we had a random distribution, we would expect an equal distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE178]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE179]'
  prefs: []
  type: TYPE_PRE
- en: Which distribution better matches our `random_counts` lead digits? To this end,
    we run a $\chi^2$-test to compare the distribution we found (`random_counts`)
    against the "natural" lead digit distribution `expected_prob_counts` and the random
    distribution `expected_random_counts`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE180]'
  prefs: []
  type: TYPE_PRE
- en: 'It turns out that there is a zero chance (`pvalue` = 0.0) that the observed
    distribution follows a "natural" distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE181]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE182]'
  prefs: []
  type: TYPE_PRE
- en: 'However, there is a 97% chance that the observed behavior follows a random
    distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE183]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE184]'
  prefs: []
  type: TYPE_PRE
- en: Hence, if you find some numbers published and doubt their validity, you can
    run the above test to check whether they are likely to be natural. Better yet,
    insist that authors use Jupyter notebooks to produce their results, such that
    you can check every step of the calculation :-)
  prefs: []
  type: TYPE_NORMAL
- en: Lessons Learned
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By specifying probabilities, one can steer fuzzing towards input features of
    interest.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning probabilities from samples allows one to focus on features that are
    common or uncommon in input samples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning probabilities from a subset of samples allows one to produce more similar
    inputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next Steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have brought together probabilities and grammars (and revisited
    parsers and grammars), we have created a foundation for many applications. Our
    next chapters will focus on
  prefs: []
  type: TYPE_NORMAL
- en: how to [*reduce* failing inputs to a minimum](Reducer.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: how to [carve](Carver.html) and [produce](APIFuzzer.html) tests at the function
    level
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: how to [automatically test (Web) user interfaces](WebFuzzer.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enjoy!
  prefs: []
  type: TYPE_NORMAL
- en: Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The idea of mining probabilities by parsing a corpus of data was first covered
    in "Learning to Fuzz: Application-Independent Fuzz Testing with Probabilistic,
    Generative Models of Input Data" [[Patra *et al*, 2016](http://mp.binaervarianz.de/TreeFuzz_TR_Nov2016.pdf)]
    which also learns and applies probabilistic rules for derivation trees. Applying
    this idea on probabilistic grammars as well as inverting probabilities or learning
    from slices was first executed in the work "Inputs from Hell: Generating Uncommon
    Inputs from Common Samples" [[Esteban Pavese *et al*, 2018](http://arxiv.org/abs/1812.07525)].'
  prefs: []
  type: TYPE_NORMAL
- en: Our exposition of Benford's law follows [this article](https://brilliant.org/wiki/benfords-law/).
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Exercise 1: Probabilistic Fuzzing with Coverage'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Create a class `ProbabilisticGrammarCoverageFuzzer` that extends `GrammarCoverageFuzzer`
    with probabilistic capabilities. The idea is to first cover all uncovered expansions
    (like `GrammarCoverageFuzzer`) and once all expansions are covered, to proceed
    by probabilities (like `ProbabilisticGrammarFuzzer`).
  prefs: []
  type: TYPE_NORMAL
- en: To this end, define new instances of the `choose_covered_node_expansion()` and
    `choose_uncovered_node_expansion()` methods that choose an expansion based on
    the given weights.
  prefs: []
  type: TYPE_NORMAL
- en: '[Use the notebook](https://mybinder.org/v2/gh/uds-se/fuzzingbook/HEAD?labpath=docs%2Fnotebooks/ProbabilisticGrammarFuzzer.ipynb#Exercises)
    to work on the exercises and see solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: If you are an advanced programmer, realize the class via *multiple inheritance*
    from `GrammarCoverageFuzzer` and `ProbabilisticGrammarFuzzer` to achieve this.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple inheritance is a tricky thing. If you have two classes $A'$ and $A''$
    which both inherit from $A$, the same method $m()$ of $A$ may be overloaded in
    both $A'$ and $A''$. If one now inherits from *both* $A'$ and $A''$, and calls
    $m()$, which of the $m()$ implementations should be called? Python "resolves"
    this conflict by simply invoking the one $m()$ method in the class one inherits
    from first.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid such conflicts, one can check whether the order in which one inherits
    makes a difference. The method `inheritance_conflicts()` compares the attributes
    with each other; if they refer to different code, you have to resolve the conflict.
  prefs: []
  type: TYPE_NORMAL
- en: '[Use the notebook](https://mybinder.org/v2/gh/uds-se/fuzzingbook/HEAD?labpath=docs%2Fnotebooks/ProbabilisticGrammarFuzzer.ipynb#Exercises)
    to work on the exercises and see solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE185]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE186]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE187]'
  prefs: []
  type: TYPE_PRE
- en: This is a method you *have* to implement for multiple inheritance besides `choose_covered_node_expansion()`
    and `choose_uncovered_node_expansion()`.
  prefs: []
  type: TYPE_NORMAL
- en: '[Use the notebook](https://mybinder.org/v2/gh/uds-se/fuzzingbook/HEAD?labpath=docs%2Fnotebooks/ProbabilisticGrammarFuzzer.ipynb#Exercises)
    to work on the exercises and see solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2: Learning from Past Bugs'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Learning from a set of inputs can be extremely valuable if one learns from *inputs
    that are known to have caused failures before.* In this exercise, you will go
    and learn distributions from past vulnerabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Download [`js-vuln-db`](https://github.com/tunz/js-vuln-db), a set of JavaScript
    engine vulnerabilities. Each vulnerability comes with code that exercises it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract all *number literals* from the code, using `re.findall()` with appropriate
    regular expressions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert these literals to (decimal) *numeric values* and count their respective
    occurrences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a grammar `RISKY_NUMBERS` that produces these numbers with probabilities
    reflecting the above counts.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Of course, there is more to vulnerabilities than just a specific numbers, but
    some numbers are more likely to induce errors than others. The next time you fuzz
    a system, do not generate numbers randomly; instead, pick one from `RISKY_NUMBERS`
    :-)
  prefs: []
  type: TYPE_NORMAL
- en: '[Use the notebook](https://mybinder.org/v2/gh/uds-se/fuzzingbook/HEAD?labpath=docs%2Fnotebooks/ProbabilisticGrammarFuzzer.ipynb#Exercises)
    to work on the exercises and see solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Creative Commons License](../Images/2f3faa36146c6fb38bbab67add09aa5f.png)
    The content of this project is licensed under the [Creative Commons Attribution-NonCommercial-ShareAlike
    4.0 International License](https://creativecommons.org/licenses/by-nc-sa/4.0/).
    The source code that is part of the content, as well as the source code used to
    format and display that content is licensed under the [MIT License](https://github.com/uds-se/fuzzingbook/blob/master/LICENSE.md#mit-license).
    [Last change: 2024-11-09 17:07:29+01:00](https://github.com/uds-se/fuzzingbook/commits/master/notebooks/ProbabilisticGrammarFuzzer.ipynb)
    • [Cite](#citation) • [Imprint](https://cispa.de/en/impressum)'
  prefs: []
  type: TYPE_IMG
- en: How to Cite this Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Andreas Zeller, Rahul Gopinath, Marcel Böhme, Gordon Fraser, and Christian
    Holler: "[Probabilistic Grammar Fuzzing](https://www.fuzzingbook.org/html/ProbabilisticGrammarFuzzer.html)".
    In Andreas Zeller, Rahul Gopinath, Marcel Böhme, Gordon Fraser, and Christian
    Holler, "[The Fuzzing Book](https://www.fuzzingbook.org/)", [https://www.fuzzingbook.org/html/ProbabilisticGrammarFuzzer.html](https://www.fuzzingbook.org/html/ProbabilisticGrammarFuzzer.html).
    Retrieved 2024-11-09 17:07:29+01:00.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE188]'
  prefs: []
  type: TYPE_PRE
